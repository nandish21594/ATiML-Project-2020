{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Feature Extraction.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4H_3pFmhtyrz",
        "colab_type": "text"
      },
      "source": [
        "# Necessary Files installation and import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T7FxP1-r-oFh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os \n",
        "os.chdir(\"/content/drive/My Drive/Colab Notebooks\")\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yIJLtdDXo-V5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "f8320a91-00d8-495d-e9ff-b4b6ceb3896d"
      },
      "source": [
        "!pip install lexical_diversity"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting lexical_diversity\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/62/37/d6f959b2255b1321b3d359d902dbd83dec3c7bb6443168d79f8911a94ae3/lexical_diversity-0.1.1-py3-none-any.whl (117kB)\n",
            "\r\u001b[K     |██▉                             | 10kB 16.9MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 20kB 2.1MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 30kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 40kB 3.1MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 51kB 2.5MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 61kB 2.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 71kB 3.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 81kB 3.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 92kB 3.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 102kB 3.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 112kB 3.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 122kB 3.4MB/s \n",
            "\u001b[?25hInstalling collected packages: lexical-diversity\n",
            "Successfully installed lexical-diversity-0.1.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QI7Q7U6C-wYP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "eaef7b63-df4c-4b2a-fdc6-aaa66fb827a3"
      },
      "source": [
        "!pip install textstat"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting textstat\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/60/af/0623a6e3adbcfda0be827664eacab5e02cd0a08d36f00013cb53784917a9/textstat-0.6.2-py3-none-any.whl (102kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 2.7MB/s \n",
            "\u001b[?25hCollecting pyphen\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/15/82/08a3629dce8d1f3d91db843bb36d4d7db6b6269d5067259613a0d5c8a9db/Pyphen-0.9.5-py2.py3-none-any.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 11.2MB/s \n",
            "\u001b[?25hInstalling collected packages: pyphen, textstat\n",
            "Successfully installed pyphen-0.9.5 textstat-0.6.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-EuzqohF6vi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "cd926d4c-00cb-4b12-81e9-e9ded10ce308"
      },
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import tokenize\n",
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from lexical_diversity import lex_div as ld\n",
        "from nltk.util import ngrams\n",
        "import gensim\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import nltk\n",
        "import os\n",
        "import codecs\n",
        "import re\n",
        "import string\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.util import ngrams\n",
        "from collections import Counter\n",
        "from collections import OrderedDict\n",
        "# import nlp\n",
        "import spacy\n",
        "from textstat.textstat import textstatistics, legacy_round\n",
        "import en_core_web_sm\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "nlp = en_core_web_sm.load()\n",
        "nlp.max_length = 3050000"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
            "  warnings.warn(\"The twython library has not been installed. \"\n",
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQzcKovVKoS2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "847042be-64f5-4b8a-f9ca-10db2bfff94d"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('vader_lexicon')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-MxWp80JDdtu",
        "colab_type": "text"
      },
      "source": [
        "# Feature extraction part\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tux2E9o00liT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#  In order to extract features from our dataset, we splitted the csv with 996 records into 6 smaller parts, \n",
        "#  like master_part1-0-100.csv , master_part2-100-300.csv , master_part3-300-500.csv, \n",
        "#  master_part4-500-700.csv, master_part5-700-950.csv, master_part6-950-996.csv , with the resources available, this spliting helped for our requirement.\n",
        "#  The results shown here are only for 300-500 data points.\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "#  This function helps us to count how many he and she pronouns are present in the dataset, first it removes irrelevant bigrams \n",
        "# and then it compares key[0] of bigram to count he and she store them in arrays.\n",
        "def gender_roles(data):\n",
        "    he_count = []\n",
        "    she_count = []\n",
        "    pronouns = ['he', 'she']\n",
        "    for idx, book in enumerate(data.bigram):\n",
        "        c1 = 0\n",
        "        c2 = 0\n",
        "        for key, val in list(book.items()):\n",
        "            if key[0] not in pronouns:\n",
        "                del book[key]\n",
        "            if key[0] == pronouns[0]:\n",
        "                c1 += 1\n",
        "\n",
        "            elif key[0] == pronouns[1]:\n",
        "                c2 += 1\n",
        "        he_count.insert(idx, c1)\n",
        "        she_count.insert(idx, c2)\n",
        "    data['He Pronoun'] = he_count\n",
        "    data['She Pronoun'] = she_count\n",
        "    return data\n",
        "\n",
        "# Reference from: https://www.geeksforgeeks.org/readability-index-pythonnlp/\n",
        "#  This function makes use of nlp object of spaCy to return documents, from which further sentences are returned.\n",
        "def sentence_break(sentence):\n",
        "    # nlp = spacy.load('en')\n",
        "    doc = nlp(sentence)\n",
        "    return doc.sents\n",
        "\n",
        "# Function is used to count total words present in all the sentences of a data point\n",
        "def word_count(sentence):\n",
        "    sentences = sentence_break(sentence)\n",
        "    words = 0\n",
        "    for sen in sentences:\n",
        "        words += len([token for token in sen])\n",
        "    return words\n",
        "\n",
        "# Function is used to count total sentences present in the data point\n",
        "def sentence_count(sentence):\n",
        "    sentences = sentence_break(sentence)\n",
        "    count = 0\n",
        "    for sen in sentences:\n",
        "        count += 1\n",
        "    return count\n",
        "\n",
        "#  This function is calculating average sentence of a data point i.e. by dividing total number of words by total number of sentences.\n",
        "def avg_sentence_length(sentence):\n",
        "    words = word_count(sentence)\n",
        "    sentences = sentence_count(sentence)\n",
        "    if sentences == 0:\n",
        "      return 0\n",
        "    else:\n",
        "      average_sentence_length = float(words / sentences)\n",
        "      return average_sentence_length\n",
        "\n",
        "# It makes use of API textstat to return sum of syllables for each word in a sentence for all the sentences present in the data point.\n",
        "def syllables_count(sentence):\n",
        "    return textstatistics().syllable_count(sentence)\n",
        "\n",
        "#  This function is calculating average syllables per word for a data point i.e. by dividing total number of syllables by total number of words.\n",
        "def avg_syllables_per_word(sentence):\n",
        "    syllable = syllables_count(sentence)\n",
        "    words = word_count(sentence)\n",
        "    if words == 0:\n",
        "      return 0\n",
        "    else:\n",
        "      average_syllables_per_word = float(syllable) / float(words)\n",
        "      return legacy_round(average_syllables_per_word, 1)\n",
        "\n",
        "# Implementing Flesch Formula:\n",
        "# Reading Ease score = 206.835 - (1.015 × ASL) - (84.6 × ASW)\n",
        "# where\n",
        "# ASL is average sentence length (number of words divided by number of sentences)\n",
        "# and ASW is average word length in syllables (number of syllables divided by number of words)\n",
        "\n",
        "def flesch_reading_score(sentence):\n",
        "    flesch_read_score = 206.835 - float(1.015 * avg_sentence_length(sentence)) - float(\n",
        "        84.6 * avg_syllables_per_word(sentence))\n",
        "    return legacy_round(flesch_read_score, 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PRfqgUrtGC12",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d93036dd-ca1f-4100-a352-11f29d3e4936"
      },
      "source": [
        "    # Read the csv file into pandas dataframe for 3rd part we have shown here.\n",
        "    data = pd.read_csv(\"master_part3_300-500.csv\", encoding=\"ISO-8859-1\")\n",
        "\n",
        "    # Replace book id with the actual name present in html so as to iterate through html files\n",
        "    data['book_id'] = data['book_id'].str.replace('.epub', '-content.html', case=False)\n",
        "    book_id_array = data['book_id'].to_numpy()\n",
        "    # print(book_id_array[0])\n",
        "\n",
        "    genre = ['guten_genre']\n",
        "\n",
        "    # Convert each genre of data dataframe into numbers\n",
        "    for x in genre:\n",
        "        le = LabelEncoder()\n",
        "        le.fit(list(data[x].values))\n",
        "        data[x] = le.transform(list(data[x]))\n",
        "\n",
        "    print('******************Dataframe after converting Genre as numbers**********************')\n",
        "    print(data)\n",
        "\n",
        "    #  Declaration of all the empty arrays \n",
        "    wordArray = []\n",
        "    contentList = []\n",
        "    index = 0\n",
        "    tokensList = []\n",
        "    sentenceList = []\n",
        "    ttrList = []\n",
        "    topicList = []\n",
        "    bigramArray = []\n",
        "    personCountList = []\n",
        "    flesch_reading_score_list = []\n",
        "\n",
        "    # Iterate through each .html files and read the contents and append to our exisitng dataframe\n",
        "    while index < len(book_id_array):\n",
        "\n",
        "        # Open the HTML file and read the contents\n",
        "        with open('/content/drive/My Drive/Colab Notebooks/Gutenberg_19th_century_English_Fiction/' + book_id_array[index], 'r', encoding='utf-8') as HTMLfile:\n",
        "          content = HTMLfile.read()\n",
        "          r = re.compile('<.*?>')\n",
        "          content_string = re.sub(r, '', content)\n",
        "\n",
        "          # Use this for finding the length of sentence\n",
        "          sent_tokens = tokenize.sent_tokenize(content_string)\n",
        "          print('*********************Sentence Tokens*************************************')\n",
        "          print('Book: ', book_id_array[index], 'Sentence Length: ', len(sent_tokens))\n",
        "\n",
        "          prunc_removed_str = re.sub('[^A-Za-z0-9]+', ' ', content_string)\n",
        "          print('Document ', index, 'is parsing..')\n",
        "\n",
        "          # Tokenize words\n",
        "          tokens = ld.tokenize(prunc_removed_str)\n",
        "          # TTR is type token ratio which we are using for calculating lexical diversity in the data point and appending it into a array.\n",
        "          ttr = ld.hdd(tokens)\n",
        "          ttrList.insert(index, ttr)\n",
        "\n",
        "          # Generating Entities for all the sentences and choosing Person Entity Label to extract number of characters feature.\n",
        "          # Reference from: https://www.geeksforgeeks.org/python-named-entity-recognition-ner-using-spacy/\n",
        "          labels = dict([(str(x), x.label_) for x in nlp(str(content_string)).ents])\n",
        "          person_count = [name for name, ent in labels.items() if ent == 'PERSON']\n",
        "          \n",
        "          # Tokenization, changing to lower case and stop word removal.\n",
        "          tokens = word_tokenize(content_string)\n",
        "          words = [word for word in tokens if word.isalpha()]\n",
        "          filtered_words = [w for w in words if not w in stopwords.words('english')]\n",
        "\n",
        "          bigram_topic = list(ngrams(filtered_words, 2))\n",
        "          bigram = Counter(list(ngrams(words, 2)))\n",
        "          wordArray.insert(index, words)\n",
        "          bigramArray.insert(index, bigram)\n",
        "          contentList.insert(index, prunc_removed_str)\n",
        "          sentenceList.insert(index, len(sent_tokens))\n",
        "          personCountList.insert(index, len(person_count))\n",
        "\n",
        "          index = index + 1\n",
        "      \n",
        "    # After removing punctuations, storing the content in dataframe\n",
        "    data['tokens'] = wordArray\n",
        "    data['bigram'] = bigramArray\n",
        "    data['content'] = contentList\n",
        "\n",
        "    # Pass the contents to SentimentIntensityAnalyser\n",
        "    sid = SentimentIntensityAnalyzer()\n",
        "    data['scores'] = data['content'].apply(lambda content: sid.polarity_scores(content))\n",
        "\n",
        "    # Compound is the value which determines if the doc is positive or negative ranges between -1 to +1, anything above 0 is positive\n",
        "    data['compound'] = data['scores'].apply(lambda score_dict: score_dict['compound'])\n",
        "\n",
        "    # Sentiment based on compound score\n",
        "    data['sentiment'] = data['compound'].apply(lambda c: 'pos' if c >= 0 else 'neg')\n",
        "    \n",
        "    # Calling Gender Roles function\n",
        "    gender_roles(data)\n",
        "\n",
        "    #  For each sentence in data's column content flesch reading score function is called and the score is stored in array later.\n",
        "    for idx, sentence in enumerate(data['content']):\n",
        "        flesch_read_score = flesch_reading_score(sentence)\n",
        "        flesch_reading_score_list.insert(idx, flesch_read_score)\n",
        "\n",
        "    data['Ease Of Readability'] = flesch_reading_score_list\n",
        "\n",
        "    # Printing data frame\n",
        "    print(data)\n",
        "\n",
        "    print('*************************Scores List*****************************')\n",
        "    # Consists of List of dict with the score having pos neg and neutral\n",
        "    scoresList = list(data['scores'])\n",
        "    print(scoresList)\n",
        "\n",
        "    # Seperate pos, neg and neutral and merge the array for vectorizing\n",
        "    posArray = [dic['pos'] for dic in scoresList]\n",
        "    negArray = [dic['neg'] for dic in scoresList]\n",
        "    neuArray = [dic['neu'] for dic in scoresList]\n",
        "    featureVec = np.vstack((posArray, neuArray, negArray)).T\n",
        "    print('Feature Vector', featureVec)\n",
        "\n",
        "    # Final Dataframe with Extracted features\n",
        "    dfFeature = pd.DataFrame(data=featureVec, columns=['postive', 'neutral', 'negative'])\n",
        "    dfFeature['sentence_length'] = sentenceList\n",
        "    dfFeature['TTR'] = ttrList\n",
        "    dfFeature['person_count'] = personCountList\n",
        "    dfFeature['she_pronoun'] = data['She Pronoun']\n",
        "    dfFeature['he_pronoun'] = data['He Pronoun']\n",
        "    dfFeature['ease_of_readability'] = data['Ease Of Readability']\n",
        "    dfFeature['class'] = data['guten_genre']\n",
        "\n",
        "    # Converting Dataframe into csv\n",
        "    dfFeature.to_csv(r'export_dataframe.csv', index=False, header=True)\n",
        "\n",
        "    print('**************************Feature Df*****************************')\n",
        "    print(dfFeature)\n",
        "\n",
        "end_time = time.time()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "******************Dataframe after converting Genre as numbers**********************\n",
            "                                             Book_Name  ...             Author_Name\n",
            "0    The Bradys Beyond Their Depth- The Great Swamp...  ...    Bird| Frederic Mayer\n",
            "1                                     Bird of Paradise  ...  Seltzer| Charles Alden\n",
            "2                                     The Giant's Robe  ...         Sabin| Edwin L.\n",
            "3                                    The Seven Secrets  ...              Anstey| F.\n",
            "4                              Five O'Clock Tea: Farce  ...       Queux| William Le\n",
            "..                                                 ...  ...                     ...\n",
            "194                                 The Mynns' Mystery  ...   Fenn| George Manville\n",
            "195                      The Man with the Double Heart  ...            Hine| Muriel\n",
            "196                                     One of My Sons  ...   Green| Anna Katharine\n",
            "197                      The Ivory Gate- a new edition  ...          Besant| Walter\n",
            "198                    Problematic Characters: A Novel  ...   Spielhagen| Friedrich\n",
            "\n",
            "[199 rows x 4 columns]\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg26925-content.html Sentence Length:  2306\n",
            "Document  0 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg27323-content.html Sentence Length:  4607\n",
            "Document  1 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg27507-content.html Sentence Length:  7764\n",
            "Document  2 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg27549-content.html Sentence Length:  4765\n",
            "Document  3 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg27709-content.html Sentence Length:  730\n",
            "Document  4 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg27838-content.html Sentence Length:  1626\n",
            "Document  5 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg27857-content.html Sentence Length:  6146\n",
            "Document  6 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg27909-content.html Sentence Length:  3791\n",
            "Document  7 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg28229-content.html Sentence Length:  5559\n",
            "Document  8 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg28236-content.html Sentence Length:  4321\n",
            "Document  9 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg28301-content.html Sentence Length:  5563\n",
            "Document  10 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg28459-content.html Sentence Length:  7052\n",
            "Document  11 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg28463-content.html Sentence Length:  10225\n",
            "Document  12 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg28465-content.html Sentence Length:  4624\n",
            "Document  13 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg28509-content.html Sentence Length:  4369\n",
            "Document  14 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg28717-content.html Sentence Length:  6120\n",
            "Document  15 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg28925-content.html Sentence Length:  11813\n",
            "Document  16 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg29071-content.html Sentence Length:  3506\n",
            "Document  17 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg29207-content.html Sentence Length:  4363\n",
            "Document  18 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg29219-content.html Sentence Length:  4595\n",
            "Document  19 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg29274-content.html Sentence Length:  3454\n",
            "Document  20 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg29315-content.html Sentence Length:  5039\n",
            "Document  21 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg29323-content.html Sentence Length:  2413\n",
            "Document  22 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg29366-content.html Sentence Length:  10980\n",
            "Document  23 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg29400-content.html Sentence Length:  4766\n",
            "Document  24 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg29570-content.html Sentence Length:  4049\n",
            "Document  25 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg29646-content.html Sentence Length:  3536\n",
            "Document  26 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg29657-content.html Sentence Length:  3730\n",
            "Document  27 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg29671-content.html Sentence Length:  4366\n",
            "Document  28 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg29695-content.html Sentence Length:  6969\n",
            "Document  29 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg29697-content.html Sentence Length:  6906\n",
            "Document  30 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg29699-content.html Sentence Length:  3499\n",
            "Document  31 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg29715-content.html Sentence Length:  3396\n",
            "Document  32 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg29726-content.html Sentence Length:  7289\n",
            "Document  33 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg29729-content.html Sentence Length:  2543\n",
            "Document  34 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg29743-content.html Sentence Length:  3976\n",
            "Document  35 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg29753-content.html Sentence Length:  5128\n",
            "Document  36 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg29760-content.html Sentence Length:  5090\n",
            "Document  37 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg29766-content.html Sentence Length:  5087\n",
            "Document  38 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg29808-content.html Sentence Length:  1159\n",
            "Document  39 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg29847-content.html Sentence Length:  8274\n",
            "Document  40 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg29851-content.html Sentence Length:  3104\n",
            "Document  41 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg29860-content.html Sentence Length:  4620\n",
            "Document  42 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg29862-content.html Sentence Length:  5398\n",
            "Document  43 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg29868-content.html Sentence Length:  5448\n",
            "Document  44 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg29880-content.html Sentence Length:  8301\n",
            "Document  45 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg29890-content.html Sentence Length:  2632\n",
            "Document  46 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg29891-content.html Sentence Length:  607\n",
            "Document  47 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg29902-content.html Sentence Length:  13155\n",
            "Document  48 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg29945-content.html Sentence Length:  5441\n",
            "Document  49 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg30031-content.html Sentence Length:  9511\n",
            "Document  50 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg30037-content.html Sentence Length:  5935\n",
            "Document  51 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg30074-content.html Sentence Length:  3544\n",
            "Document  52 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg30075-content.html Sentence Length:  2542\n",
            "Document  53 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg30087-content.html Sentence Length:  2807\n",
            "Document  54 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg30089-content.html Sentence Length:  2274\n",
            "Document  55 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg30090-content.html Sentence Length:  3181\n",
            "Document  56 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg30095-content.html Sentence Length:  7504\n",
            "Document  57 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg30106-content.html Sentence Length:  8308\n",
            "Document  58 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg30108-content.html Sentence Length:  6919\n",
            "Document  59 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg30110-content.html Sentence Length:  7713\n",
            "Document  60 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg30111-content.html Sentence Length:  8555\n",
            "Document  61 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg30115-content.html Sentence Length:  10312\n",
            "Document  62 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg30137-content.html Sentence Length:  5278\n",
            "Document  63 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg30183-content.html Sentence Length:  2679\n",
            "Document  64 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg30193-content.html Sentence Length:  4055\n",
            "Document  65 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg30228-content.html Sentence Length:  4612\n",
            "Document  66 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg30245-content.html Sentence Length:  6993\n",
            "Document  67 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg30247-content.html Sentence Length:  7074\n",
            "Document  68 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg30263-content.html Sentence Length:  4611\n",
            "Document  69 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg30286-content.html Sentence Length:  7343\n",
            "Document  70 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg30302-content.html Sentence Length:  1192\n",
            "Document  71 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg30436-content.html Sentence Length:  7260\n",
            "Document  72 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg30448-content.html Sentence Length:  2614\n",
            "Document  73 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg30464-content.html Sentence Length:  2361\n",
            "Document  74 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg30466-content.html Sentence Length:  1620\n",
            "Document  75 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg30483-content.html Sentence Length:  3575\n",
            "Document  76 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg305-content.html Sentence Length:  7000\n",
            "Document  77 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg30600-content.html Sentence Length:  3056\n",
            "Document  78 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg30617-content.html Sentence Length:  3895\n",
            "Document  79 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg30618-content.html Sentence Length:  5670\n",
            "Document  80 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg30627-content.html Sentence Length:  5940\n",
            "Document  81 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg30640-content.html Sentence Length:  1425\n",
            "Document  82 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg30648-content.html Sentence Length:  3069\n",
            "Document  83 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg30668-content.html Sentence Length:  2084\n",
            "Document  84 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg30692-content.html Sentence Length:  9635\n",
            "Document  85 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg30704-content.html Sentence Length:  3043\n",
            "Document  86 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg30713-content.html Sentence Length:  5647\n",
            "Document  87 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg30724-content.html Sentence Length:  6673\n",
            "Document  88 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg30733-content.html Sentence Length:  3523\n",
            "Document  89 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg30736-content.html Sentence Length:  5442\n",
            "Document  90 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg30776-content.html Sentence Length:  2773\n",
            "Document  91 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg30817-content.html Sentence Length:  3604\n",
            "Document  92 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg30838-content.html Sentence Length:  4891\n",
            "Document  93 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg30980-content.html Sentence Length:  3919\n",
            "Document  94 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg31004-content.html Sentence Length:  7121\n",
            "Document  95 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg31083-content.html Sentence Length:  498\n",
            "Document  96 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg31241-content.html Sentence Length:  8141\n",
            "Document  97 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg31561-content.html Sentence Length:  3571\n",
            "Document  98 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg31563-content.html Sentence Length:  1485\n",
            "Document  99 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg31568-content.html Sentence Length:  1552\n",
            "Document  100 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg31578-content.html Sentence Length:  5815\n",
            "Document  101 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg31668-content.html Sentence Length:  8385\n",
            "Document  102 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg31813-content.html Sentence Length:  2517\n",
            "Document  103 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg31820-content.html Sentence Length:  9070\n",
            "Document  104 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg31825-content.html Sentence Length:  7065\n",
            "Document  105 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg31915-content.html Sentence Length:  4951\n",
            "Document  106 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg31927-content.html Sentence Length:  1707\n",
            "Document  107 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg31984-content.html Sentence Length:  9761\n",
            "Document  108 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg32042-content.html Sentence Length:  5079\n",
            "Document  109 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg32082-content.html Sentence Length:  3263\n",
            "Document  110 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg32114-content.html Sentence Length:  8410\n",
            "Document  111 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg32115-content.html Sentence Length:  4640\n",
            "Document  112 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg32116-content.html Sentence Length:  2940\n",
            "Document  113 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg32132-content.html Sentence Length:  4434\n",
            "Document  114 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg32424-content.html Sentence Length:  6695\n",
            "Document  115 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg32428-content.html Sentence Length:  6409\n",
            "Document  116 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg32516-content.html Sentence Length:  3074\n",
            "Document  117 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg32560-content.html Sentence Length:  6626\n",
            "Document  118 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg32576-content.html Sentence Length:  1699\n",
            "Document  119 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg32692-content.html Sentence Length:  6959\n",
            "Document  120 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg32693-content.html Sentence Length:  3443\n",
            "Document  121 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg32732-content.html Sentence Length:  4971\n",
            "Document  122 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg32757-content.html Sentence Length:  1563\n",
            "Document  123 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg32826-content.html Sentence Length:  7015\n",
            "Document  124 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg32840-content.html Sentence Length:  10824\n",
            "Document  125 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg32882-content.html Sentence Length:  7963\n",
            "Document  126 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg32915-content.html Sentence Length:  4957\n",
            "Document  127 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg32917-content.html Sentence Length:  5499\n",
            "Document  128 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg32929-content.html Sentence Length:  6469\n",
            "Document  129 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg32936-content.html Sentence Length:  5749\n",
            "Document  130 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg33004-content.html Sentence Length:  4739\n",
            "Document  131 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg33019-content.html Sentence Length:  2471\n",
            "Document  132 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg33081-content.html Sentence Length:  8761\n",
            "Document  133 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg33099-content.html Sentence Length:  2614\n",
            "Document  134 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg33195-content.html Sentence Length:  5732\n",
            "Document  135 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg33312-content.html Sentence Length:  7484\n",
            "Document  136 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg33401-content.html Sentence Length:  1587\n",
            "Document  137 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg33423-content.html Sentence Length:  4448\n",
            "Document  138 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg33468-content.html Sentence Length:  6452\n",
            "Document  139 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg33469-content.html Sentence Length:  6748\n",
            "Document  140 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg33482-content.html Sentence Length:  2365\n",
            "Document  141 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg33490-content.html Sentence Length:  9876\n",
            "Document  142 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg33498-content.html Sentence Length:  4299\n",
            "Document  143 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg33556-content.html Sentence Length:  7544\n",
            "Document  144 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg33565-content.html Sentence Length:  6675\n",
            "Document  145 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg33604-content.html Sentence Length:  10321\n",
            "Document  146 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg33664-content.html Sentence Length:  7690\n",
            "Document  147 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg33695-content.html Sentence Length:  949\n",
            "Document  148 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg33715-content.html Sentence Length:  9260\n",
            "Document  149 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg33747-content.html Sentence Length:  4659\n",
            "Document  150 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg33759-content.html Sentence Length:  3772\n",
            "Document  151 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg33780-content.html Sentence Length:  6579\n",
            "Document  152 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg33867-content.html Sentence Length:  4122\n",
            "Document  153 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg33892-content.html Sentence Length:  5309\n",
            "Document  154 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg33945-content.html Sentence Length:  4179\n",
            "Document  155 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg33959-content.html Sentence Length:  5191\n",
            "Document  156 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg33973-content.html Sentence Length:  1152\n",
            "Document  157 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg33989-content.html Sentence Length:  4549\n",
            "Document  158 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg34034-content.html Sentence Length:  6638\n",
            "Document  159 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg34035-content.html Sentence Length:  7404\n",
            "Document  160 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg34046-content.html Sentence Length:  7372\n",
            "Document  161 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg34088-content.html Sentence Length:  6186\n",
            "Document  162 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg34138-content.html Sentence Length:  4805\n",
            "Document  163 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg34140-content.html Sentence Length:  8572\n",
            "Document  164 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg34142-content.html Sentence Length:  6081\n",
            "Document  165 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg34164-content.html Sentence Length:  0\n",
            "Document  166 is parsing..\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:155: DeprecationWarning: generator 'ngrams' raised StopIteration\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:156: DeprecationWarning: generator 'ngrams' raised StopIteration\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg34166-content.html Sentence Length:  5858\n",
            "Document  167 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg34168-content.html Sentence Length:  5351\n",
            "Document  168 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg34202-content.html Sentence Length:  3984\n",
            "Document  169 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg34244-content.html Sentence Length:  7928\n",
            "Document  170 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg34246-content.html Sentence Length:  11453\n",
            "Document  171 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg34248-content.html Sentence Length:  9537\n",
            "Document  172 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg34277-content.html Sentence Length:  6066\n",
            "Document  173 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg34402-content.html Sentence Length:  6701\n",
            "Document  174 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg34415-content.html Sentence Length:  6669\n",
            "Document  175 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg34428-content.html Sentence Length:  7277\n",
            "Document  176 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg34482-content.html Sentence Length:  4612\n",
            "Document  177 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg34500-content.html Sentence Length:  8276\n",
            "Document  178 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg34567-content.html Sentence Length:  9265\n",
            "Document  179 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg34575-content.html Sentence Length:  7167\n",
            "Document  180 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg34587-content.html Sentence Length:  2960\n",
            "Document  181 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg34598-content.html Sentence Length:  10322\n",
            "Document  182 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg34599-content.html Sentence Length:  2278\n",
            "Document  183 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg34609-content.html Sentence Length:  9240\n",
            "Document  184 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg34616-content.html Sentence Length:  4634\n",
            "Document  185 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg34619-content.html Sentence Length:  3490\n",
            "Document  186 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg34628-content.html Sentence Length:  5443\n",
            "Document  187 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg34629-content.html Sentence Length:  997\n",
            "Document  188 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg34657-content.html Sentence Length:  3997\n",
            "Document  189 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg34658-content.html Sentence Length:  4126\n",
            "Document  190 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg34659-content.html Sentence Length:  4124\n",
            "Document  191 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg34662-content.html Sentence Length:  219\n",
            "Document  192 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg34663-content.html Sentence Length:  6921\n",
            "Document  193 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg34664-content.html Sentence Length:  5342\n",
            "Document  194 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg34709-content.html Sentence Length:  8567\n",
            "Document  195 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg34710-content.html Sentence Length:  5361\n",
            "Document  196 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg34738-content.html Sentence Length:  4027\n",
            "Document  197 is parsing..\n",
            "*********************Sentence Tokens*************************************\n",
            "Book:  pg34748-content.html Sentence Length:  10601\n",
            "Document  198 is parsing..\n",
            "                                             Book_Name  ... Ease Of Readability\n",
            "0    The Bradys Beyond Their Depth- The Great Swamp...  ...               82.36\n",
            "1                                     Bird of Paradise  ...               83.92\n",
            "2                                     The Giant's Robe  ...               80.86\n",
            "3                                    The Seven Secrets  ...               70.93\n",
            "4                              Five O'Clock Tea: Farce  ...               88.92\n",
            "..                                                 ...  ...                 ...\n",
            "194                                 The Mynns' Mystery  ...               92.17\n",
            "195                      The Man with the Double Heart  ...               83.96\n",
            "196                                     One of My Sons  ...               77.87\n",
            "197                      The Ivory Gate- a new edition  ...               81.69\n",
            "198                    Problematic Characters: A Novel  ...               77.68\n",
            "\n",
            "[199 rows x 13 columns]\n",
            "*************************Scores List*****************************\n",
            "[{'neg': 0.116, 'neu': 0.781, 'pos': 0.103, 'compound': -0.9999}, {'neg': 0.088, 'neu': 0.713, 'pos': 0.2, 'compound': 1.0}, {'neg': 0.102, 'neu': 0.754, 'pos': 0.143, 'compound': 1.0}, {'neg': 0.126, 'neu': 0.739, 'pos': 0.135, 'compound': 1.0}, {'neg': 0.076, 'neu': 0.754, 'pos': 0.17, 'compound': 1.0}, {'neg': 0.129, 'neu': 0.739, 'pos': 0.133, 'compound': 0.9996}, {'neg': 0.079, 'neu': 0.768, 'pos': 0.152, 'compound': 1.0}, {'neg': 0.088, 'neu': 0.789, 'pos': 0.122, 'compound': 1.0}, {'neg': 0.086, 'neu': 0.804, 'pos': 0.11, 'compound': 1.0}, {'neg': 0.087, 'neu': 0.746, 'pos': 0.167, 'compound': 1.0}, {'neg': 0.09, 'neu': 0.746, 'pos': 0.164, 'compound': 1.0}, {'neg': 0.085, 'neu': 0.765, 'pos': 0.15, 'compound': 1.0}, {'neg': 0.112, 'neu': 0.726, 'pos': 0.162, 'compound': 1.0}, {'neg': 0.089, 'neu': 0.719, 'pos': 0.192, 'compound': 1.0}, {'neg': 0.1, 'neu': 0.765, 'pos': 0.135, 'compound': 1.0}, {'neg': 0.115, 'neu': 0.722, 'pos': 0.163, 'compound': 1.0}, {'neg': 0.102, 'neu': 0.734, 'pos': 0.164, 'compound': 1.0}, {'neg': 0.078, 'neu': 0.807, 'pos': 0.115, 'compound': 1.0}, {'neg': 0.083, 'neu': 0.761, 'pos': 0.156, 'compound': 1.0}, {'neg': 0.089, 'neu': 0.759, 'pos': 0.152, 'compound': 1.0}, {'neg': 0.103, 'neu': 0.769, 'pos': 0.128, 'compound': 1.0}, {'neg': 0.082, 'neu': 0.765, 'pos': 0.152, 'compound': 1.0}, {'neg': 0.091, 'neu': 0.763, 'pos': 0.146, 'compound': 1.0}, {'neg': 0.094, 'neu': 0.764, 'pos': 0.143, 'compound': 1.0}, {'neg': 0.119, 'neu': 0.782, 'pos': 0.099, 'compound': -1.0}, {'neg': 0.073, 'neu': 0.787, 'pos': 0.14, 'compound': 1.0}, {'neg': 0.091, 'neu': 0.807, 'pos': 0.102, 'compound': 1.0}, {'neg': 0.079, 'neu': 0.762, 'pos': 0.16, 'compound': 1.0}, {'neg': 0.115, 'neu': 0.737, 'pos': 0.148, 'compound': 1.0}, {'neg': 0.114, 'neu': 0.758, 'pos': 0.128, 'compound': 1.0}, {'neg': 0.117, 'neu': 0.772, 'pos': 0.112, 'compound': -0.9999}, {'neg': 0.102, 'neu': 0.755, 'pos': 0.143, 'compound': 1.0}, {'neg': 0.088, 'neu': 0.736, 'pos': 0.176, 'compound': 1.0}, {'neg': 0.097, 'neu': 0.766, 'pos': 0.137, 'compound': 1.0}, {'neg': 0.092, 'neu': 0.747, 'pos': 0.161, 'compound': 1.0}, {'neg': 0.081, 'neu': 0.773, 'pos': 0.146, 'compound': 1.0}, {'neg': 0.095, 'neu': 0.738, 'pos': 0.167, 'compound': 1.0}, {'neg': 0.1, 'neu': 0.758, 'pos': 0.142, 'compound': 1.0}, {'neg': 0.103, 'neu': 0.743, 'pos': 0.154, 'compound': 1.0}, {'neg': 0.088, 'neu': 0.774, 'pos': 0.138, 'compound': 1.0}, {'neg': 0.091, 'neu': 0.78, 'pos': 0.128, 'compound': 1.0}, {'neg': 0.09, 'neu': 0.81, 'pos': 0.1, 'compound': 1.0}, {'neg': 0.073, 'neu': 0.774, 'pos': 0.153, 'compound': 1.0}, {'neg': 0.107, 'neu': 0.721, 'pos': 0.172, 'compound': 1.0}, {'neg': 0.086, 'neu': 0.748, 'pos': 0.166, 'compound': 1.0}, {'neg': 0.111, 'neu': 0.74, 'pos': 0.149, 'compound': 1.0}, {'neg': 0.122, 'neu': 0.749, 'pos': 0.129, 'compound': 1.0}, {'neg': 0.109, 'neu': 0.73, 'pos': 0.161, 'compound': 1.0}, {'neg': 0.09, 'neu': 0.776, 'pos': 0.134, 'compound': 1.0}, {'neg': 0.086, 'neu': 0.764, 'pos': 0.15, 'compound': 1.0}, {'neg': 0.09, 'neu': 0.771, 'pos': 0.138, 'compound': 1.0}, {'neg': 0.118, 'neu': 0.774, 'pos': 0.108, 'compound': -1.0}, {'neg': 0.095, 'neu': 0.753, 'pos': 0.152, 'compound': 1.0}, {'neg': 0.108, 'neu': 0.771, 'pos': 0.121, 'compound': 0.9999}, {'neg': 0.083, 'neu': 0.783, 'pos': 0.134, 'compound': 1.0}, {'neg': 0.093, 'neu': 0.774, 'pos': 0.133, 'compound': 1.0}, {'neg': 0.083, 'neu': 0.77, 'pos': 0.147, 'compound': 1.0}, {'neg': 0.101, 'neu': 0.759, 'pos': 0.14, 'compound': 1.0}, {'neg': 0.098, 'neu': 0.777, 'pos': 0.124, 'compound': 1.0}, {'neg': 0.093, 'neu': 0.771, 'pos': 0.136, 'compound': 1.0}, {'neg': 0.102, 'neu': 0.741, 'pos': 0.157, 'compound': 1.0}, {'neg': 0.141, 'neu': 0.711, 'pos': 0.148, 'compound': 1.0}, {'neg': 0.091, 'neu': 0.754, 'pos': 0.155, 'compound': 1.0}, {'neg': 0.137, 'neu': 0.688, 'pos': 0.174, 'compound': 1.0}, {'neg': 0.058, 'neu': 0.799, 'pos': 0.143, 'compound': 1.0}, {'neg': 0.093, 'neu': 0.74, 'pos': 0.168, 'compound': 1.0}, {'neg': 0.102, 'neu': 0.76, 'pos': 0.138, 'compound': 1.0}, {'neg': 0.068, 'neu': 0.772, 'pos': 0.161, 'compound': 1.0}, {'neg': 0.125, 'neu': 0.72, 'pos': 0.156, 'compound': 1.0}, {'neg': 0.078, 'neu': 0.783, 'pos': 0.139, 'compound': 1.0}, {'neg': 0.094, 'neu': 0.762, 'pos': 0.144, 'compound': 1.0}, {'neg': 0.093, 'neu': 0.765, 'pos': 0.142, 'compound': 1.0}, {'neg': 0.097, 'neu': 0.723, 'pos': 0.18, 'compound': 1.0}, {'neg': 0.09, 'neu': 0.774, 'pos': 0.137, 'compound': 1.0}, {'neg': 0.094, 'neu': 0.752, 'pos': 0.154, 'compound': 1.0}, {'neg': 0.124, 'neu': 0.78, 'pos': 0.096, 'compound': -1.0}, {'neg': 0.097, 'neu': 0.765, 'pos': 0.138, 'compound': 1.0}, {'neg': 0.112, 'neu': 0.744, 'pos': 0.144, 'compound': 1.0}, {'neg': 0.101, 'neu': 0.742, 'pos': 0.157, 'compound': 1.0}, {'neg': 0.063, 'neu': 0.773, 'pos': 0.164, 'compound': 1.0}, {'neg': 0.097, 'neu': 0.763, 'pos': 0.14, 'compound': 1.0}, {'neg': 0.087, 'neu': 0.771, 'pos': 0.142, 'compound': 1.0}, {'neg': 0.076, 'neu': 0.82, 'pos': 0.104, 'compound': 1.0}, {'neg': 0.084, 'neu': 0.788, 'pos': 0.128, 'compound': 1.0}, {'neg': 0.103, 'neu': 0.754, 'pos': 0.143, 'compound': 1.0}, {'neg': 0.091, 'neu': 0.747, 'pos': 0.162, 'compound': 1.0}, {'neg': 0.071, 'neu': 0.789, 'pos': 0.14, 'compound': 1.0}, {'neg': 0.096, 'neu': 0.772, 'pos': 0.131, 'compound': 1.0}, {'neg': 0.118, 'neu': 0.743, 'pos': 0.138, 'compound': 1.0}, {'neg': 0.092, 'neu': 0.752, 'pos': 0.156, 'compound': 1.0}, {'neg': 0.083, 'neu': 0.777, 'pos': 0.14, 'compound': 1.0}, {'neg': 0.06, 'neu': 0.801, 'pos': 0.139, 'compound': 1.0}, {'neg': 0.105, 'neu': 0.718, 'pos': 0.177, 'compound': 1.0}, {'neg': 0.081, 'neu': 0.802, 'pos': 0.118, 'compound': 1.0}, {'neg': 0.109, 'neu': 0.749, 'pos': 0.142, 'compound': 1.0}, {'neg': 0.091, 'neu': 0.77, 'pos': 0.14, 'compound': 1.0}, {'neg': 0.08, 'neu': 0.819, 'pos': 0.101, 'compound': 0.9997}, {'neg': 0.077, 'neu': 0.798, 'pos': 0.125, 'compound': 1.0}, {'neg': 0.091, 'neu': 0.762, 'pos': 0.147, 'compound': 1.0}, {'neg': 0.095, 'neu': 0.786, 'pos': 0.119, 'compound': 1.0}, {'neg': 0.117, 'neu': 0.759, 'pos': 0.124, 'compound': 0.9999}, {'neg': 0.114, 'neu': 0.768, 'pos': 0.119, 'compound': 1.0}, {'neg': 0.113, 'neu': 0.733, 'pos': 0.154, 'compound': 1.0}, {'neg': 0.082, 'neu': 0.753, 'pos': 0.164, 'compound': 1.0}, {'neg': 0.086, 'neu': 0.747, 'pos': 0.167, 'compound': 1.0}, {'neg': 0.122, 'neu': 0.722, 'pos': 0.157, 'compound': 1.0}, {'neg': 0.092, 'neu': 0.774, 'pos': 0.133, 'compound': 1.0}, {'neg': 0.102, 'neu': 0.775, 'pos': 0.122, 'compound': 1.0}, {'neg': 0.119, 'neu': 0.738, 'pos': 0.144, 'compound': 1.0}, {'neg': 0.088, 'neu': 0.772, 'pos': 0.14, 'compound': 1.0}, {'neg': 0.099, 'neu': 0.75, 'pos': 0.151, 'compound': 1.0}, {'neg': 0.084, 'neu': 0.788, 'pos': 0.128, 'compound': 1.0}, {'neg': 0.078, 'neu': 0.756, 'pos': 0.166, 'compound': 1.0}, {'neg': 0.101, 'neu': 0.771, 'pos': 0.128, 'compound': 1.0}, {'neg': 0.078, 'neu': 0.792, 'pos': 0.13, 'compound': 1.0}, {'neg': 0.093, 'neu': 0.763, 'pos': 0.144, 'compound': 1.0}, {'neg': 0.092, 'neu': 0.753, 'pos': 0.155, 'compound': 1.0}, {'neg': 0.069, 'neu': 0.794, 'pos': 0.137, 'compound': 1.0}, {'neg': 0.101, 'neu': 0.75, 'pos': 0.149, 'compound': 1.0}, {'neg': 0.104, 'neu': 0.754, 'pos': 0.142, 'compound': 1.0}, {'neg': 0.096, 'neu': 0.753, 'pos': 0.151, 'compound': 1.0}, {'neg': 0.089, 'neu': 0.763, 'pos': 0.148, 'compound': 1.0}, {'neg': 0.073, 'neu': 0.771, 'pos': 0.156, 'compound': 1.0}, {'neg': 0.122, 'neu': 0.751, 'pos': 0.128, 'compound': 0.9997}, {'neg': 0.08, 'neu': 0.769, 'pos': 0.151, 'compound': 1.0}, {'neg': 0.1, 'neu': 0.749, 'pos': 0.15, 'compound': 1.0}, {'neg': 0.096, 'neu': 0.734, 'pos': 0.17, 'compound': 1.0}, {'neg': 0.101, 'neu': 0.729, 'pos': 0.17, 'compound': 1.0}, {'neg': 0.117, 'neu': 0.744, 'pos': 0.138, 'compound': 1.0}, {'neg': 0.101, 'neu': 0.744, 'pos': 0.154, 'compound': 1.0}, {'neg': 0.098, 'neu': 0.726, 'pos': 0.177, 'compound': 1.0}, {'neg': 0.07, 'neu': 0.794, 'pos': 0.136, 'compound': 1.0}, {'neg': 0.112, 'neu': 0.79, 'pos': 0.097, 'compound': -1.0}, {'neg': 0.101, 'neu': 0.751, 'pos': 0.148, 'compound': 1.0}, {'neg': 0.079, 'neu': 0.8, 'pos': 0.122, 'compound': 1.0}, {'neg': 0.101, 'neu': 0.713, 'pos': 0.186, 'compound': 1.0}, {'neg': 0.111, 'neu': 0.71, 'pos': 0.179, 'compound': 1.0}, {'neg': 0.108, 'neu': 0.72, 'pos': 0.172, 'compound': 1.0}, {'neg': 0.117, 'neu': 0.759, 'pos': 0.124, 'compound': 1.0}, {'neg': 0.096, 'neu': 0.748, 'pos': 0.156, 'compound': 1.0}, {'neg': 0.11, 'neu': 0.745, 'pos': 0.146, 'compound': 1.0}, {'neg': 0.092, 'neu': 0.77, 'pos': 0.137, 'compound': 1.0}, {'neg': 0.087, 'neu': 0.755, 'pos': 0.158, 'compound': 1.0}, {'neg': 0.107, 'neu': 0.756, 'pos': 0.138, 'compound': 1.0}, {'neg': 0.098, 'neu': 0.752, 'pos': 0.15, 'compound': 1.0}, {'neg': 0.08, 'neu': 0.737, 'pos': 0.183, 'compound': 1.0}, {'neg': 0.085, 'neu': 0.763, 'pos': 0.152, 'compound': 1.0}, {'neg': 0.113, 'neu': 0.722, 'pos': 0.165, 'compound': 1.0}, {'neg': 0.111, 'neu': 0.767, 'pos': 0.123, 'compound': 0.9998}, {'neg': 0.085, 'neu': 0.789, 'pos': 0.126, 'compound': 1.0}, {'neg': 0.085, 'neu': 0.728, 'pos': 0.187, 'compound': 1.0}, {'neg': 0.091, 'neu': 0.787, 'pos': 0.122, 'compound': 1.0}, {'neg': 0.089, 'neu': 0.761, 'pos': 0.15, 'compound': 1.0}, {'neg': 0.095, 'neu': 0.767, 'pos': 0.138, 'compound': 1.0}, {'neg': 0.119, 'neu': 0.756, 'pos': 0.124, 'compound': 1.0}, {'neg': 0.165, 'neu': 0.689, 'pos': 0.146, 'compound': -1.0}, {'neg': 0.075, 'neu': 0.788, 'pos': 0.136, 'compound': 1.0}, {'neg': 0.12, 'neu': 0.731, 'pos': 0.149, 'compound': 1.0}, {'neg': 0.109, 'neu': 0.672, 'pos': 0.219, 'compound': 1.0}, {'neg': 0.09, 'neu': 0.768, 'pos': 0.142, 'compound': 1.0}, {'neg': 0.074, 'neu': 0.783, 'pos': 0.143, 'compound': 1.0}, {'neg': 0.087, 'neu': 0.823, 'pos': 0.091, 'compound': 0.9965}, {'neg': 0.11, 'neu': 0.732, 'pos': 0.158, 'compound': 1.0}, {'neg': 0.115, 'neu': 0.729, 'pos': 0.156, 'compound': 1.0}, {'neg': 0.118, 'neu': 0.724, 'pos': 0.158, 'compound': 1.0}, {'neg': 0.113, 'neu': 0.741, 'pos': 0.146, 'compound': 1.0}, {'neg': 0.0, 'neu': 0.0, 'pos': 0.0, 'compound': 0.0}, {'neg': 0.089, 'neu': 0.76, 'pos': 0.151, 'compound': 1.0}, {'neg': 0.119, 'neu': 0.723, 'pos': 0.157, 'compound': 1.0}, {'neg': 0.086, 'neu': 0.781, 'pos': 0.133, 'compound': 1.0}, {'neg': 0.117, 'neu': 0.727, 'pos': 0.156, 'compound': 1.0}, {'neg': 0.117, 'neu': 0.736, 'pos': 0.147, 'compound': 1.0}, {'neg': 0.123, 'neu': 0.732, 'pos': 0.145, 'compound': 1.0}, {'neg': 0.103, 'neu': 0.762, 'pos': 0.135, 'compound': 1.0}, {'neg': 0.096, 'neu': 0.768, 'pos': 0.137, 'compound': 1.0}, {'neg': 0.104, 'neu': 0.764, 'pos': 0.132, 'compound': 1.0}, {'neg': 0.109, 'neu': 0.764, 'pos': 0.127, 'compound': 1.0}, {'neg': 0.101, 'neu': 0.733, 'pos': 0.165, 'compound': 1.0}, {'neg': 0.124, 'neu': 0.731, 'pos': 0.145, 'compound': 1.0}, {'neg': 0.094, 'neu': 0.79, 'pos': 0.116, 'compound': 1.0}, {'neg': 0.114, 'neu': 0.727, 'pos': 0.16, 'compound': 1.0}, {'neg': 0.085, 'neu': 0.788, 'pos': 0.127, 'compound': 1.0}, {'neg': 0.103, 'neu': 0.733, 'pos': 0.164, 'compound': 1.0}, {'neg': 0.095, 'neu': 0.756, 'pos': 0.149, 'compound': 1.0}, {'neg': 0.123, 'neu': 0.727, 'pos': 0.15, 'compound': 1.0}, {'neg': 0.116, 'neu': 0.715, 'pos': 0.169, 'compound': 1.0}, {'neg': 0.092, 'neu': 0.737, 'pos': 0.171, 'compound': 1.0}, {'neg': 0.092, 'neu': 0.743, 'pos': 0.165, 'compound': 1.0}, {'neg': 0.108, 'neu': 0.737, 'pos': 0.155, 'compound': 1.0}, {'neg': 0.083, 'neu': 0.771, 'pos': 0.146, 'compound': 1.0}, {'neg': 0.093, 'neu': 0.743, 'pos': 0.164, 'compound': 1.0}, {'neg': 0.117, 'neu': 0.751, 'pos': 0.132, 'compound': 1.0}, {'neg': 0.073, 'neu': 0.749, 'pos': 0.179, 'compound': 1.0}, {'neg': 0.125, 'neu': 0.75, 'pos': 0.126, 'compound': 0.9984}, {'neg': 0.116, 'neu': 0.733, 'pos': 0.151, 'compound': 1.0}, {'neg': 0.098, 'neu': 0.734, 'pos': 0.168, 'compound': 1.0}, {'neg': 0.113, 'neu': 0.761, 'pos': 0.126, 'compound': 1.0}, {'neg': 0.076, 'neu': 0.76, 'pos': 0.164, 'compound': 1.0}, {'neg': 0.091, 'neu': 0.733, 'pos': 0.175, 'compound': 1.0}]\n",
            "Feature Vector [[0.103 0.781 0.116]\n",
            " [0.2   0.713 0.088]\n",
            " [0.143 0.754 0.102]\n",
            " [0.135 0.739 0.126]\n",
            " [0.17  0.754 0.076]\n",
            " [0.133 0.739 0.129]\n",
            " [0.152 0.768 0.079]\n",
            " [0.122 0.789 0.088]\n",
            " [0.11  0.804 0.086]\n",
            " [0.167 0.746 0.087]\n",
            " [0.164 0.746 0.09 ]\n",
            " [0.15  0.765 0.085]\n",
            " [0.162 0.726 0.112]\n",
            " [0.192 0.719 0.089]\n",
            " [0.135 0.765 0.1  ]\n",
            " [0.163 0.722 0.115]\n",
            " [0.164 0.734 0.102]\n",
            " [0.115 0.807 0.078]\n",
            " [0.156 0.761 0.083]\n",
            " [0.152 0.759 0.089]\n",
            " [0.128 0.769 0.103]\n",
            " [0.152 0.765 0.082]\n",
            " [0.146 0.763 0.091]\n",
            " [0.143 0.764 0.094]\n",
            " [0.099 0.782 0.119]\n",
            " [0.14  0.787 0.073]\n",
            " [0.102 0.807 0.091]\n",
            " [0.16  0.762 0.079]\n",
            " [0.148 0.737 0.115]\n",
            " [0.128 0.758 0.114]\n",
            " [0.112 0.772 0.117]\n",
            " [0.143 0.755 0.102]\n",
            " [0.176 0.736 0.088]\n",
            " [0.137 0.766 0.097]\n",
            " [0.161 0.747 0.092]\n",
            " [0.146 0.773 0.081]\n",
            " [0.167 0.738 0.095]\n",
            " [0.142 0.758 0.1  ]\n",
            " [0.154 0.743 0.103]\n",
            " [0.138 0.774 0.088]\n",
            " [0.128 0.78  0.091]\n",
            " [0.1   0.81  0.09 ]\n",
            " [0.153 0.774 0.073]\n",
            " [0.172 0.721 0.107]\n",
            " [0.166 0.748 0.086]\n",
            " [0.149 0.74  0.111]\n",
            " [0.129 0.749 0.122]\n",
            " [0.161 0.73  0.109]\n",
            " [0.134 0.776 0.09 ]\n",
            " [0.15  0.764 0.086]\n",
            " [0.138 0.771 0.09 ]\n",
            " [0.108 0.774 0.118]\n",
            " [0.152 0.753 0.095]\n",
            " [0.121 0.771 0.108]\n",
            " [0.134 0.783 0.083]\n",
            " [0.133 0.774 0.093]\n",
            " [0.147 0.77  0.083]\n",
            " [0.14  0.759 0.101]\n",
            " [0.124 0.777 0.098]\n",
            " [0.136 0.771 0.093]\n",
            " [0.157 0.741 0.102]\n",
            " [0.148 0.711 0.141]\n",
            " [0.155 0.754 0.091]\n",
            " [0.174 0.688 0.137]\n",
            " [0.143 0.799 0.058]\n",
            " [0.168 0.74  0.093]\n",
            " [0.138 0.76  0.102]\n",
            " [0.161 0.772 0.068]\n",
            " [0.156 0.72  0.125]\n",
            " [0.139 0.783 0.078]\n",
            " [0.144 0.762 0.094]\n",
            " [0.142 0.765 0.093]\n",
            " [0.18  0.723 0.097]\n",
            " [0.137 0.774 0.09 ]\n",
            " [0.154 0.752 0.094]\n",
            " [0.096 0.78  0.124]\n",
            " [0.138 0.765 0.097]\n",
            " [0.144 0.744 0.112]\n",
            " [0.157 0.742 0.101]\n",
            " [0.164 0.773 0.063]\n",
            " [0.14  0.763 0.097]\n",
            " [0.142 0.771 0.087]\n",
            " [0.104 0.82  0.076]\n",
            " [0.128 0.788 0.084]\n",
            " [0.143 0.754 0.103]\n",
            " [0.162 0.747 0.091]\n",
            " [0.14  0.789 0.071]\n",
            " [0.131 0.772 0.096]\n",
            " [0.138 0.743 0.118]\n",
            " [0.156 0.752 0.092]\n",
            " [0.14  0.777 0.083]\n",
            " [0.139 0.801 0.06 ]\n",
            " [0.177 0.718 0.105]\n",
            " [0.118 0.802 0.081]\n",
            " [0.142 0.749 0.109]\n",
            " [0.14  0.77  0.091]\n",
            " [0.101 0.819 0.08 ]\n",
            " [0.125 0.798 0.077]\n",
            " [0.147 0.762 0.091]\n",
            " [0.119 0.786 0.095]\n",
            " [0.124 0.759 0.117]\n",
            " [0.119 0.768 0.114]\n",
            " [0.154 0.733 0.113]\n",
            " [0.164 0.753 0.082]\n",
            " [0.167 0.747 0.086]\n",
            " [0.157 0.722 0.122]\n",
            " [0.133 0.774 0.092]\n",
            " [0.122 0.775 0.102]\n",
            " [0.144 0.738 0.119]\n",
            " [0.14  0.772 0.088]\n",
            " [0.151 0.75  0.099]\n",
            " [0.128 0.788 0.084]\n",
            " [0.166 0.756 0.078]\n",
            " [0.128 0.771 0.101]\n",
            " [0.13  0.792 0.078]\n",
            " [0.144 0.763 0.093]\n",
            " [0.155 0.753 0.092]\n",
            " [0.137 0.794 0.069]\n",
            " [0.149 0.75  0.101]\n",
            " [0.142 0.754 0.104]\n",
            " [0.151 0.753 0.096]\n",
            " [0.148 0.763 0.089]\n",
            " [0.156 0.771 0.073]\n",
            " [0.128 0.751 0.122]\n",
            " [0.151 0.769 0.08 ]\n",
            " [0.15  0.749 0.1  ]\n",
            " [0.17  0.734 0.096]\n",
            " [0.17  0.729 0.101]\n",
            " [0.138 0.744 0.117]\n",
            " [0.154 0.744 0.101]\n",
            " [0.177 0.726 0.098]\n",
            " [0.136 0.794 0.07 ]\n",
            " [0.097 0.79  0.112]\n",
            " [0.148 0.751 0.101]\n",
            " [0.122 0.8   0.079]\n",
            " [0.186 0.713 0.101]\n",
            " [0.179 0.71  0.111]\n",
            " [0.172 0.72  0.108]\n",
            " [0.124 0.759 0.117]\n",
            " [0.156 0.748 0.096]\n",
            " [0.146 0.745 0.11 ]\n",
            " [0.137 0.77  0.092]\n",
            " [0.158 0.755 0.087]\n",
            " [0.138 0.756 0.107]\n",
            " [0.15  0.752 0.098]\n",
            " [0.183 0.737 0.08 ]\n",
            " [0.152 0.763 0.085]\n",
            " [0.165 0.722 0.113]\n",
            " [0.123 0.767 0.111]\n",
            " [0.126 0.789 0.085]\n",
            " [0.187 0.728 0.085]\n",
            " [0.122 0.787 0.091]\n",
            " [0.15  0.761 0.089]\n",
            " [0.138 0.767 0.095]\n",
            " [0.124 0.756 0.119]\n",
            " [0.146 0.689 0.165]\n",
            " [0.136 0.788 0.075]\n",
            " [0.149 0.731 0.12 ]\n",
            " [0.219 0.672 0.109]\n",
            " [0.142 0.768 0.09 ]\n",
            " [0.143 0.783 0.074]\n",
            " [0.091 0.823 0.087]\n",
            " [0.158 0.732 0.11 ]\n",
            " [0.156 0.729 0.115]\n",
            " [0.158 0.724 0.118]\n",
            " [0.146 0.741 0.113]\n",
            " [0.    0.    0.   ]\n",
            " [0.151 0.76  0.089]\n",
            " [0.157 0.723 0.119]\n",
            " [0.133 0.781 0.086]\n",
            " [0.156 0.727 0.117]\n",
            " [0.147 0.736 0.117]\n",
            " [0.145 0.732 0.123]\n",
            " [0.135 0.762 0.103]\n",
            " [0.137 0.768 0.096]\n",
            " [0.132 0.764 0.104]\n",
            " [0.127 0.764 0.109]\n",
            " [0.165 0.733 0.101]\n",
            " [0.145 0.731 0.124]\n",
            " [0.116 0.79  0.094]\n",
            " [0.16  0.727 0.114]\n",
            " [0.127 0.788 0.085]\n",
            " [0.164 0.733 0.103]\n",
            " [0.149 0.756 0.095]\n",
            " [0.15  0.727 0.123]\n",
            " [0.169 0.715 0.116]\n",
            " [0.171 0.737 0.092]\n",
            " [0.165 0.743 0.092]\n",
            " [0.155 0.737 0.108]\n",
            " [0.146 0.771 0.083]\n",
            " [0.164 0.743 0.093]\n",
            " [0.132 0.751 0.117]\n",
            " [0.179 0.749 0.073]\n",
            " [0.126 0.75  0.125]\n",
            " [0.151 0.733 0.116]\n",
            " [0.168 0.734 0.098]\n",
            " [0.126 0.761 0.113]\n",
            " [0.164 0.76  0.076]\n",
            " [0.175 0.733 0.091]]\n",
            "**************************Feature Df*****************************\n",
            "     postive  neutral  negative  ...  he_pronoun  ease_of_readability  class\n",
            "0      0.103    0.781     0.116  ...         163                82.36      1\n",
            "1      0.200    0.713     0.088  ...         251                83.92      3\n",
            "2      0.143    0.754     0.102  ...         507                80.86      3\n",
            "3      0.135    0.739     0.126  ...         210                70.93      1\n",
            "4      0.170    0.754     0.076  ...          29                88.92      3\n",
            "..       ...      ...       ...  ...         ...                  ...    ...\n",
            "194    0.151    0.733     0.116  ...         305                92.17      1\n",
            "195    0.168    0.734     0.098  ...         348                83.96      3\n",
            "196    0.126    0.761     0.113  ...         377                77.87      1\n",
            "197    0.164    0.760     0.076  ...         211                81.69      3\n",
            "198    0.175    0.733     0.091  ...         430                77.68      3\n",
            "\n",
            "[199 rows x 10 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ihq1euBfHXff",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c95c384d-b475-4501-a56a-a6abece12b89"
      },
      "source": [
        "print((end_time-start_time) / 60)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "252.70055793921154\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}